{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from CSV file\n",
    "try:\n",
    "    df = pd.read_csv('../data/synthetic/synthetic_network.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "    exit()\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The file is empty or invalid.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Identify unique nodes and create a mapping for indices\n",
    "nodos = sorted(set(df['origin']).union(set(df['destination'])))\n",
    "node_to_index = {node: idx for idx, node in enumerate(nodos)}\n",
    "n = len(nodos)\n",
    "\n",
    "# Initialize the cost matrix with infinity\n",
    "cost_matrix = np.full((n, n), np.inf)\n",
    "\n",
    "\n",
    "# Set diagonal to 0 (self-costs)\n",
    "np.fill_diagonal(cost_matrix, 0)\n",
    "\n",
    "# Fill the cost matrix with the values from the CSV\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        origen = row['origin']\n",
    "        destino = row['destination']\n",
    "        costo = float(row['weight'])\n",
    "        cost_matrix[node_to_index[origen], node_to_index[destino]] = costo\n",
    "    except KeyError:\n",
    "        print(\"Error: Missing columns 'origin', 'destination', or 'weight'.\")\n",
    "        exit()\n",
    "    except ValueError:\n",
    "        print(f\"Error: Invalid cost value on row {_}.\")\n",
    "        exit()\n",
    "\n",
    "# Display the cost matrix\n",
    "print(\"Cost Matrix:\")\n",
    "print(cost_matrix)\n",
    "\n",
    "np.save('../data/synthetic/cost_matrix.npy', cost_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline training to learn graph properties without source/destination nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "C = cost_matrix\n",
    "n = C.shape[0]  # NÃºmero de nodos\n",
    "print(\"Numero de nodos\")\n",
    "print (n)\n",
    "# Vectorizar la matriz de costes\n",
    "C_flat = C.flatten()\n",
    "C_flat[np.isinf(C_flat)] = 1e6  # Reemplazar infinito por un valor grande\n",
    "flattened_cost_matrix = C_flat\n",
    "num_nodes = n\n",
    "\n",
    "def offline_loss(y_true, y_pred, num_nodes, flattened_cost_matrix):\n",
    "    \n",
    "    predicted_arc_values = y_pred[:, :num_nodes * num_nodes]\n",
    "    reshaped_values = tf.reshape(predicted_arc_values, (-1, num_nodes, num_nodes))\n",
    "\n",
    "    # Path cost\n",
    "    cost_term = tf.reduce_sum(flattened_cost_matrix * predicted_arc_values)\n",
    "\n",
    "    # Outgoing edge constraint\n",
    "    outgoing_sums = tf.reduce_sum(reshaped_values, axis=2)\n",
    "    outgoing_edge_penalty = tf.reduce_sum(tf.square(outgoing_sums - 1))\n",
    "\n",
    "    # Incoming edge constraint\n",
    "    incoming_sums = tf.reduce_sum(reshaped_values, axis=1)\n",
    "    incoming_edge_penalty = tf.reduce_sum(tf.square(incoming_sums - 1))\n",
    "\n",
    "    # Binary values constraint\n",
    "    binary_penalty = tf.reduce_sum(predicted_arc_values * (1 - predicted_arc_values))\n",
    "\n",
    "    loss = cost_term + 100 * outgoing_edge_penalty + 100 * incoming_edge_penalty + 10 * binary_penalty\n",
    "    return loss\n",
    "\n",
    "# Define offline training model\n",
    "offline_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(num_nodes * num_nodes,)),\n",
    "        tf.keras.layers.Dense(num_nodes * num_nodes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile offline model\n",
    "offline_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                          loss=lambda y_true, y_pred: offline_loss(y_true, y_pred, num_nodes, flattened_cost_matrix))\n",
    "\n",
    "# Train offline model with early stopping\n",
    "offline_input = np.expand_dims(flattened_cost_matrix, axis=0)\n",
    "offline_model.fit(\n",
    "        offline_input, \n",
    "        offline_input, \n",
    "        epochs=500, \n",
    "        verbose=1, \n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "# Save pre-trained model\n",
    "offline_model.save('../models/offline_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune pretrained model for specific origin and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "origin = 0\n",
    "destination = 3\n",
    "\n",
    "# Load pre-trained offline model\n",
    "offline_model = tf.keras.models.load_model('../models/offline_model.keras', safe_mode=False,\n",
    "                                           custom_objects={'offline_loss': offline_loss})\n",
    "\n",
    "# Placeholder for cost matrix and input vector creation\n",
    "C = cost_matrix\n",
    "n = C.shape[0]\n",
    "C_flat = C.flatten()\n",
    "C_flat[np.isinf(C_flat)] = 1e6\n",
    "\n",
    "# Create input vector with source and destination nodes\n",
    "def create_input_vector(C_flat, origin, destination, n):\n",
    "    origin_vector = np.zeros(n)\n",
    "    origin_vector[origin] = 1\n",
    "    destination_vector = np.zeros(n)\n",
    "    destination_vector[destination] = 1\n",
    "    return np.concatenate([C_flat, origin_vector, destination_vector])\n",
    "\n",
    "# Fine-tuning for specific source and destination nodes\n",
    "def energy_loss_with_input_vectors(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the loss for a predicted path in a graph with specified input vectors.\n",
    "\n",
    "    This function is similar to the offline loss, but it takes into account the input vectors\n",
    "    for the origin and destination nodes. It applies penalties to deviations from the\n",
    "    constraints for path cost, outgoing edges, incoming edges, binary values, and the\n",
    "    source and destination nodes.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth values (not utilized in this computation).\n",
    "        y_pred: Predicted values, containing arc values, origin vector, and destination vector.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The computed loss value as a TensorFlow tensor.\n",
    "    \"\"\"\n",
    "    predicted_arcs = y_pred[:, :n * n]\n",
    "    origin_vec = y_pred[:, n * n:n * n + n]\n",
    "    destination_vec = y_pred[:, n * n + n:]\n",
    "\n",
    "    # Path cost\n",
    "    path_cost = tf.reduce_sum(C_flat * predicted_arcs)\n",
    "\n",
    "    # Outgoing and Incoming edge constraints\n",
    "    reshaped_arcs = tf.reshape(predicted_arcs, (-1, n, n))\n",
    "    outgoing_edges = tf.reduce_sum(reshaped_arcs, axis=2) - 1\n",
    "    incoming_edges = tf.reduce_sum(reshaped_arcs, axis=1) - 1\n",
    "    edge_constraints = tf.reduce_sum(tf.square(outgoing_edges)) + tf.reduce_sum(tf.square(incoming_edges))\n",
    "\n",
    "    # Binary values constraint\n",
    "    binary_constraint = tf.reduce_sum(predicted_arcs * (1 - predicted_arcs))\n",
    "\n",
    "    # Source and Destination constraints\n",
    "    source_index = tf.argmax(origin_vec, axis=1)\n",
    "    source_outflow = tf.gather(reshaped_arcs, source_index, batch_dims=1)\n",
    "    source_constraint = tf.reduce_sum(source_outflow, axis=1) - 1\n",
    "\n",
    "    destination_index = tf.argmax(destination_vec, axis=1)\n",
    "    destination_inflow = tf.gather(tf.transpose(reshaped_arcs, perm=[0, 2, 1]), destination_index, batch_dims=1)\n",
    "    destination_constraint = tf.reduce_sum(destination_inflow, axis=1) - 1\n",
    "\n",
    "    # Compute total loss\n",
    "    loss = (path_cost \n",
    "            + 10 * edge_constraints \n",
    "            + 10 * binary_constraint \n",
    "            + 100 * tf.reduce_sum(tf.square(source_constraint)) \n",
    "            + 100 * tf.reduce_sum(tf.square(destination_constraint)))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Define full model for fine-tuning\n",
    "input_dim = len(C_flat) + 2 * n\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Load weights from offline model and adjust\n",
    "offline_weights = offline_model.layers[0].get_weights()\n",
    "offline_kernel, offline_bias = offline_weights\n",
    "\n",
    "# Adjust weights to match the new input dimension\n",
    "new_kernel = np.zeros((input_dim, input_dim))  # Create new kernel with the correct size\n",
    "new_bias = np.zeros(input_dim)  # Adjust bias size\n",
    "\n",
    "# Copy offline weights for the C_flat portion\n",
    "new_kernel[:len(C_flat), :len(C_flat)] = offline_kernel\n",
    "new_bias[:len(C_flat)] = offline_bias\n",
    "\n",
    "# Set adjusted weights into the fine-tuning model\n",
    "model.layers[0].set_weights([new_kernel, new_bias])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=energy_loss_with_input_vectors)\n",
    "\n",
    "# Create input vector\n",
    "input_vector = create_input_vector(C_flat, origin, destination, n)\n",
    "input_data = np.expand_dims(input_vector, axis=0)\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(input_data, input_data, epochs=500, verbose=1)\n",
    "\n",
    "# Predictions and results\n",
    "predictions = model.predict(input_data)[0]\n",
    "arc_values = predictions[:n * n]\n",
    "arc_matrix = arc_values.reshape((n, n)) > 0.5\n",
    "\n",
    "print(\"Matriz de arcos seleccionados:\")\n",
    "print(arc_matrix.astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
